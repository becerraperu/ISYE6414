---
title: "HW5"
author: "Chong Zhang"
date: "November 26, 2018"
output: html_document
---

```{r cache=TRUE}
library(dplyr)
library(ggplot2)
library(glmnet)
library(MASS)
library(corrplot)


load(file = 'AmesHousing.RData')
attach(house)
scatterplotMatrix(~SalePrice+MS.Zoning+Lot.Area+Lot.Shape+Land.Contour,data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+Neighborhood+Condition.1+Bldg.Type+House.Style,data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+Overall.Qual+Overall.Cond+Year.Built+Year.Remod.Add, smooth=FALSE)
scatterplotMatrix(~SalePrice+Roof.Style++Mas.Vnr.Type+Mas.Vnr.Area+Exter.Qual,data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+Exter.Cond+Foundation+Bsmt.Qual+Bsmt.Cond,data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+BsmtFin.SF+Total.Bsmt.SF+Heating.QC+Central.Air, data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+X1st.Flr.SF+Gr.Liv.Area+Bedroom.AbvGr+Kitchen.Qual,data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+TotRms.AbvGrd+Fireplaces+Fireplace.Qu+Garage.Type,data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+Garage.Finish+Garage.Cars+Garage.Area+Paved.Drive,data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+Wood.Deck.SF+Open.Porch.SF+Enclosed.Porch+X3Ssn.Porch,data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+Screen.Porch+Pool.Area+Fence+Sale.Type,data = house, smooth=FALSE)
scatterplotMatrix(~SalePrice+Sale.Condition+Yr.Sold+Bath,data = house, smooth=FALSE)

```

The distribution of SalePrice is hightly skewed. There are couple of predicting variables shows high level of linearality with SalePrice. Some do not show significant level of linearity. 

(b)

```{r cache=TRUE}
X = model.matrix(lm(house$SalePrice~., data = house))[,-1]
X = cbind(house$SalePrice, X)
corrplot(cor(X), tl.cex = 0.1)
```

There are 18 predictors that correlate strongly with SalePrice.
Yes, there is evidence showing multicollinearity based on the graph. For example, Garage.Cars tend to corrlated strongly with Garage.Area. Because the larger the size of the garage, the more cars you can put in the garage.

Question 2:
```{r cache=TRUE}
model = lm(SalePrice~., data = house)
plot(house$Lot.Area,model$residuals)
abline(h=0,col='red')
plot(house$SalePrice, model$residuals)
abline(h=0,col='red')
plot(house$Year.Built, model$residuals)
abline(h=0,col='red')
plot(house$Mas.Vnr.Area, model$residuals)
abline(h=0,col='red')
plot(house$Garage.Area, model$residuals)
abline(h=0,col='red')
plot(house$Open.Porch.SF, model$residuals)
abline(h=0,col='red')
plot(fitted(model), model$residuals)
abline(h=0,col='red')
```

I have plotted couple of predicting variables against the residules of the model. It seems that overall, the variance is constant for different values of the predicting variables.
However when I plot SalePrice against the residuels, I found that there was a departure at the right tail of the graph.

(b)
```{r cache=TRUE}
bc = boxcox(model)
best.lam = bc$x[which(bc$y==max(bc$y))]
```

best lambda is 0.141414141

(c)
```{r cache=TRUE}
model_log = lm(log(SalePrice)~., data = house)
plot(x=fitted(model_log),y=residuals(model_log))
abline(h=0, col='red')


```

Yes, it seems there is at least 3 outliers.

(d)
```{r cache=TRUE}
library(car)
cook = cooks.distance(model)
cook = as.data.frame(cbind(index=1:length(house$SalePrice), cooksd = cook))
p = ggplot(data = cook,aes(x=index, y = cooksd)) + geom_point(col='red')+xlab('Index')+ylab('Cook\'s distance')+theme(axis.title = element_text(size=12))
p = p + geom_hline(yintercept = 1, col='blue')
p = p + geom_text(aes(label = ifelse(cooksd>1, index,'')),hjust=-0.1, vjust=0.01)
p
influencePlot(model_log)
plot(cook)
```

After compute the cook's distance, I found that there is no points whose cook's distance is larger than 1. So no observation was removed.

(e)
```{r cache=TRUE}
qqnorm(model$residuals)
qqline(model$residuals)

```

There are departures at both ends of the QQ plot, indicating that the normality assumption may not hold. Thus the estimation of and inference on the regression coefficients might not be reliable.

Question 3: Variable Selection via Stepwise Regression
(a)
```{r cache=TRUE}
summary(model)
drop_result = drop1(model)
a = which(drop_result$AIC==max(drop_result$AIC))
b = which(drop_result$AIC==min(drop_result$AIC))
drop_result[a,]
drop_result[b,]
```


It seems that by dropping Yr.sold can improve AIC the most. Dropping Neighborhood affect the AIC the most.


(b)
```{r cache=TRUE}
final_model = stepAIC(model)
length(final_model$coefficients)
```

There are 81 coefficients in the final model. The final AIC is 56620.99.

(c)
```{r cache=TRUE}
forward_model = step(model,direction = 'forward')
length(forward_model$coefficients)
backward_model = step(model, direction = 'backward')
length(backward_model$coefficients)
both = step(model, direction = 'both')
length(both$coefficients)

anova(forward_model,both)
anova(backward_model,both)

```

Forward model selected 96 coefficients, backward model selected 81 coefficients, whereas the forward-backward model selected 81 coefficients. Partial F-test by ANOVA suggesting that the extra number of variables included in the forward_model do not contribute significant information to the SalePrice once all the variables in the both model have been taken into consideration. Thus their coeiffients tend to be 0.

Question 4: Variable Selection via LASSO and Elastic Net

(a)
```{r cache=TRUE}
lasso_cv = cv.glmnet(x = model.matrix(~.,house[-47]),y = house$SalePrice,alpha = 1,nfolds = 10)
lasso = glmnet(x = model.matrix(~.,house[-47]),y = house$SalePrice,alpha = 1, nlambda = 100)
plot(lasso, xvar='lambda',label=TRUE, lwd=2)
abline(v=log(lasso_cv$lambda.min),col='black',lty=2,lwd=2)

x = which(lasso_cv$lambda==lasso_cv$lambda.min)
lasso_cv$nzero[x]

```

The best lambda is ```r round(lasso_cv$lambda.min)```. 82 out of 95 model coefficients are not zero in te fit.

(b)

```{r cache=TRUE}
enet_cv = cv.glmnet(x = model.matrix(~.,house[-47]),y = house$SalePrice,alpha = 0.5,nfolds = 10)
enet = glmnet(x = model.matrix(~.,house[-47]),y = house$SalePrice,alpha = 1, nlambda = 100)
plot(enet, xvar='lambda',label=TRUE, lwd=2)
abline(v=log(enet_cv$lambda.min),col='black',lty=2,lwd=2)

x = which(enet_cv$lambda==enet_cv$lambda.min)
enet_cv$nzero[x]
```

The best lambda is ```r round(enet_cv$lambda.min)```. 78 out of 95 model coefficients are not zero in te fit. There are 4 parameters less in the elastic net model compared to the lasso model. elastic net is a combination of lasso and ridge regression. because ridge does not perform variable selection, it makes the elastic net model with more variables.

(c)

```{r cache=TRUE}
coef(lasso,s=lasso_cv$lambda.min)
coef(enet,s=enet_cv$lambda.min)


```

Question 5: Variable Selection via Group LASSO

(a)
```{r cache=TRUE}
library(gglasso)
set.seed(1)

group = c(48,rep(1,2),2,3,rep(4,3),rep(5,27), rep(6,3), rep(7,4), rep(8,2), 9, 10, 11, 12, rep(13,2), rep(14,2), 15, 16, 17,

          rep(18,3), 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, rep(32,3), 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,

          43, rep(44,3), rep(45,3), rep(46,4), 47)

cv = cv.gglasso(model.matrix(model_log), house$SalePrice, group, loss="ls"); cv$lambda.min

glassomdl = gglasso(model.matrix(model_log), house$SalePrice, group, lambda = cv$lambda.min)

nnzero(glassomdl$beta)
```


()